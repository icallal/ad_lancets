{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heq754/.conda/envs/lowpyAD/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dementia_utils as dem\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from mlutils import calc_results\n",
    "from mlutils import pick_threshold\n",
    "\n",
    "from flaml import AutoML\n",
    "\n",
    "FLAML_TIME_BUDGET = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg parser\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--fold_index\", type=int, required=True)\n",
    "    parser.add_argument( # this modifies the field ids\n",
    "        \"--experiment\",\n",
    "        type=str,\n",
    "        help=\"\"\"Experiment name. Options:\n",
    "        'age_alone' - only age\n",
    "        'all_demographics' - age, sex, apoe, education?\n",
    "        'age_sex_lancet2024' - lancet factors + age and sex\n",
    "        'demographics_lancet2024' - lancet factors + all demographics\n",
    "        \"\"\",\n",
    "        required = False\n",
    "    )\n",
    "\n",
    "    # these modify the patient-snp arrays\n",
    "    parser.add_argument(\"--age_cutoff\", type=int, default=None, help=\"Options: False (allages), True (65up)\", required=True)\n",
    "    parser.add_argument(\"--snps\", type=str, default=None, help=\"Options: 'all_snps', 'apoe_only', 'LDE', polygenic_risk_score\", required=True)\n",
    "    parser.add_argument(\"--alzheimer_only\", type=int, default=None, help=\"Options: True ('AD'), False('ACD')\", required=True)\n",
    "    parser.add_argument(\"--model\", type=str, default=None, help=\"Model to use. Options: 'lgbm', 'lrl1'\", required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Parse the arguments\n",
    "    experiment = args.experiment\n",
    "\n",
    "    if age_cutoff == 0:\n",
    "        age_cutoff = False # allages\n",
    "    elif age_cutoff == 1: \n",
    "        age_cutoff = True # 65up\n",
    "\n",
    "    snps = args.snps\n",
    "    \n",
    "    if args.alzheimers_only == 0:\n",
    "        alzheimers_only = False\n",
    "    elif args.alzheimers_only == 1:\n",
    "        alzheimers_only = True\n",
    "    else:\n",
    "        print(\"predict_alzheimers_only must be 0 or 1\")\n",
    "        sys.exit()\n",
    "    \n",
    "    model = args.model\n",
    "    \n",
    "    return (\n",
    "        experiment,\n",
    "        age_cutoff,\n",
    "        snps, \n",
    "        alzheimers_only,\n",
    "        model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alzheimers cases only? or not...\n",
    "\n",
    "def pull_alzheimer_only(df, ddf):\n",
    "    # pull cases\n",
    "    alz_eid, _, _ = dem.pull_dementia_cases(ddf, alzheimers_only = True) # a tuple of both_eid, date_df, exclude_df\n",
    "\n",
    "    dem_eid , _ , _ = dem.pull_dementia_cases(ddf, alzheimers_only = False) # a tuple of both_eid, date_df, exclude_df\n",
    "\n",
    "    # add an extra column to the df with whether the patient is a case (1) or control (0) or neither (2)\n",
    "    df['groups'] = 0 \n",
    "\n",
    "    # assign 2 to excluded patients\n",
    "    df.loc[df['IID'].isin(dem_eid), 'groups'] = 2.0\n",
    "\n",
    "    # assign 1 to alz patients \n",
    "    df.loc[df['IID'].isin(alz_eid), 'groups'] = 1.0\n",
    "\n",
    "    # drop excluded\n",
    "    df = df[df.groups.isin([0, 1])]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def pull_all_dementia(df, ddf):\n",
    "    # all dementias\n",
    "    dem_eid , _ , _ = dem.pull_dementia_cases(ddf, alzheimers_only = False) # a tuple of both_eid, date_df, exclude_df\n",
    "\n",
    "    # add an extra column to the df with whether the patient is a case (1) or control (0) or neither (2)\n",
    "    df['groups'] = 0 \n",
    "\n",
    "    # assign 2 to excluded patients\n",
    "    df.loc[df['IID'].isin(dem_eid), 'groups'] = 1.0\n",
    "\n",
    "    # drop excluded\n",
    "    df = df[df.groups.isin([0, 1])]\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_apoe(df):\n",
    "    # encode APOE\n",
    "    df[\"e3/e3\"] = 0\n",
    "    df[\"e3/e4\"] = 0\n",
    "    df[\"e2/e3\"] = 0\n",
    "    df[\"e2/e4\"] = 0\n",
    "    df[\"e4/e4\"] = 0\n",
    "    df[\"e2/e2\"] = 0\n",
    "\n",
    "    df.loc[\n",
    "        (df.rs429358_T == 2) & (df.rs7412_C == 2), \"e3/e3\"\n",
    "    ] = 1\n",
    "    df.loc[\n",
    "        (df.rs429358_T == 1) & (df.rs7412_C == 2), \"e3/e4\"\n",
    "    ] = 1\n",
    "    df.loc[\n",
    "        (df.rs429358_T == 2) & (df.rs7412_C == 1), \"e2/e3\"\n",
    "    ] = 1\n",
    "    df.loc[\n",
    "        (df.rs429358_T == 1) & (df.rs7412_C == 1), \"e2/e4\"\n",
    "    ] = 1\n",
    "    df.loc[\n",
    "        (df.rs429358_T == 0) & (df.rs7412_C == 2), \"e4/e4\"\n",
    "    ] = 1\n",
    "    df.loc[\n",
    "        (df.rs429358_T == 2) & (df.rs7412_C == 0), \"e2/e2\"\n",
    "    ] = 1\n",
    "\n",
    "    df = df.drop(columns = [\"rs429358_T\", \"rs7412_C\"]) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dfs to read in? \n",
    "def load_datasets(snps, age_cutoff=True, alzheimers_only=True): \n",
    "    # df for which snps\n",
    "    if snps == 'apoe':\n",
    "        df = pd.read_parquet('raw_apoe_AD.parquet', engine = 'fastparquet') # apoe only\n",
    "    elif snps == 'all_snps':\n",
    "        df = pd.read_parquet('raw_allsnps_AD.parquet', engine = 'fastparquet') # all snps\n",
    "        # encode apoe\n",
    "        df = encode_apoe(df)\n",
    "    elif snps == 'LDE':\n",
    "        df = pd.read_parquet('raw_unlinkedsnps_AD.parquet', engine = 'fastparquet') # LDE snps\n",
    "        df_apoe = pd.read_parquet('raw_apoe_AD.parquet', engine = 'fastparquet') # apoe only\n",
    "        df = df.merge(df_apoe, on=\"IID\", how=\"left\") # add apoe snps to LDE snps by merging on IID\n",
    "\n",
    "    elif snps == 'apoe_stratified': \n",
    "        df = pd.read_parquet('raw_apoestrat_AD.parquet', engine = 'fastparquet')\n",
    "        df_apoe = pd.read_parquet('raw_apoe_AD.parquet', engine = 'fastparquet') # apoe only\n",
    "        df = df.merge(df_apoe, on=\"IID\", how=\"left\") # add apoe snps to LDE snps by merging on IID\n",
    "    elif snps == 'polygenic_risk_score':\n",
    "        df = pd.read_parquet('../../../randy/proj_idp/tidy_data/prs_Alz/prs_Alz.parquet', engine = 'fastparquet')\n",
    "        if 'eid' in df.columns:\n",
    "            df = df.rename(columns={'eid': 'IID'})\n",
    "    else: \n",
    "        print(\"snps must be one of: 'apoe', 'all_snps', 'LDE', 'polygenic_risk_score'\")\n",
    "        sys.exit()\n",
    "    \n",
    "    print('loaded_datasets')\n",
    "    print(df.head())\n",
    "\n",
    "    # which ages\n",
    "    ddf = pd.read_parquet('allcausedementia.parquet', engine = 'fastparquet') # age, sex, all-patients\n",
    "    if age_cutoff == True: # only patients 65 and older\n",
    "        ddf = ddf[ddf['curr_age'] >= 65]\n",
    "\n",
    "        df = df[df['IID'].isin(ddf['eid'])]\n",
    " \n",
    "    # assign groups \n",
    "    if alzheimers_only: \n",
    "        df = pull_alzheimer_only(df, ddf)\n",
    "    else:\n",
    "        df = pull_all_dementia(df, ddf)\n",
    "\n",
    "    \n",
    "    # subset to practical df (ONLY FOR TESTING)\n",
    "    case = df[df.groups == 1]\n",
    "    case = case.iloc[:50, :]\n",
    "    case.shape\n",
    "\n",
    "    ctrl = df[df.groups == 0]\n",
    "    ctrl = ctrl.iloc[:50, :]\n",
    "    ctrl.shape\n",
    "\n",
    "    pracdf = pd.concat([case, ctrl])\n",
    "    \n",
    "    nrows = 100\n",
    "    ncol = 50\n",
    "\n",
    "\n",
    "    # defining x, y\n",
    "    print(pracdf.head())\n",
    "    drop_cols = ['FID', 'PAT', 'MAT', 'SEX', 'PHENOTYPE', 'groups']\n",
    "    cols_to_drop = [col for col in drop_cols if col in pracdf.columns]\n",
    "    X = pracdf.drop(columns=cols_to_drop)\n",
    "    X = X.iloc[:nrows, :ncol]\n",
    "    print(X.head())\n",
    "    y = pracdf.values[:nrows, -1]\n",
    "\n",
    "    #X = df.drop(columns = ['FID', 'IID', 'PAT', 'MAT', 'SEX', 'PHENOTYPE', 'groups'])\n",
    "    #y = df.values[:, -1]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_path(age_cutoff, snps, alzheimers_only): # does this need a default arg? \n",
    "    root = \"./results\"\n",
    "    \n",
    "    age_cond = \"65up\" if age_cutoff else \"allages\"\n",
    "    alz_cond = \"AD\" if alzheimers_only else \"ACD\"\n",
    "\n",
    "    path = f\"{root}/{age_cond}/{alz_cond}/{snps}\"  \n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lancet_vars():\n",
    "    \"\"\"\n",
    "    Returns two lists of variables related to a study.\n",
    "\n",
    "    The first list, `lancet_vars`, contains a mix of categorical and continuous variables.\n",
    "    The second list, `continuous_lancet_vars`, contains only continuous variables.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - lancet_vars (list of str): A list of variable identifiers and names.\n",
    "            - continuous_lancet_vars (list of str): A list of continuous variable identifiers.\n",
    "    \"\"\"\n",
    "    lancet_vars = [\n",
    "        \"4700-0.0\",\n",
    "        \"5901-0.0\",\n",
    "        \"30780-0.0\",\n",
    "        \"head_injury\",\n",
    "        \"22038-0.0\",\n",
    "        \"20161-0.0\",\n",
    "        \"alcohol_consumption\",\n",
    "        \"hypertension\",\n",
    "        \"obesity\",\n",
    "        \"diabetes\",\n",
    "        \"hearing_loss\",\n",
    "        \"depression\",\n",
    "        \"freq_friends_family_visit\",\n",
    "        \"24012-0.0\",\n",
    "        \"24018-0.0\",\n",
    "        \"24019-0.0\",\n",
    "        \"24006-0.0\",\n",
    "        \"24015-0.0\",\n",
    "        \"24011-0.0\",\n",
    "        \"2020-0.0_-3.0\",\n",
    "        \"2020-0.0_-1.0\",\n",
    "        \"2020-0.0_0.0\",\n",
    "        \"2020-0.0_1.0\",\n",
    "        \"2020-0.0_nan\",\n",
    "    ]\n",
    "    continuous_lancet_vars = [\n",
    "        \"4700-0.0\",\n",
    "        \"5901-0.0\",\n",
    "        \"30780-0.0\",\n",
    "        \"22038-0.0\",\n",
    "        \"20161-0.0\",\n",
    "        \"24012-0.0\",\n",
    "        \"24018-0.0\",\n",
    "        \"24019-0.0\",\n",
    "        \"24006-0.0\",\n",
    "        \"24015-0.0\",\n",
    "        \"24011-0.0\",\n",
    "    ]\n",
    "    return lancet_vars, continuous_lancet_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>6138-0.0</th>\n",
       "      <th>6138-0.1</th>\n",
       "      <th>6138-0.2</th>\n",
       "      <th>6138-0.3</th>\n",
       "      <th>6138-0.4</th>\n",
       "      <th>6138-0.5</th>\n",
       "      <th>6138-1.0</th>\n",
       "      <th>6138-1.1</th>\n",
       "      <th>6138-1.2</th>\n",
       "      <th>...</th>\n",
       "      <th>21003-2.0_squared</th>\n",
       "      <th>21003-3.0_squared</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>21000-0.0</th>\n",
       "      <th>21000-1.0</th>\n",
       "      <th>21000-2.0</th>\n",
       "      <th>845-0.0</th>\n",
       "      <th>845-1.0</th>\n",
       "      <th>845-2.0</th>\n",
       "      <th>curr_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6025251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4761.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6025268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6025273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000054</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       eid  6138-0.0  6138-0.1  6138-0.2  6138-0.3  6138-0.4  6138-0.5  \\\n",
       "0  6025251       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1  6025268       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2  6025273       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3  1000054      -7.0       NaN       NaN       NaN       NaN       NaN   \n",
       "4  1000078       1.0       2.0       4.0       NaN       NaN       NaN   \n",
       "\n",
       "   6138-1.0  6138-1.1  6138-1.2  ...  21003-2.0_squared  21003-3.0_squared  \\\n",
       "0       NaN       NaN       NaN  ...             4761.0                NaN   \n",
       "1       NaN       NaN       NaN  ...                NaN                NaN   \n",
       "2       NaN       NaN       NaN  ...                NaN                NaN   \n",
       "3       NaN       NaN       NaN  ...                NaN                NaN   \n",
       "4       NaN       NaN       NaN  ...                NaN                NaN   \n",
       "\n",
       "   31-0.0  21000-0.0  21000-1.0  21000-2.0  845-0.0  845-1.0  845-2.0  \\\n",
       "0     NaN        NaN        NaN        NaN      NaN      NaN      NaN   \n",
       "1     NaN        NaN        NaN        NaN      NaN      NaN      NaN   \n",
       "2     NaN     1001.0        NaN        NaN      NaN      NaN      NaN   \n",
       "3     0.0     1001.0        NaN        NaN     17.0      NaN      NaN   \n",
       "4     0.0     1001.0        NaN        NaN      NaN      NaN      NaN   \n",
       "\n",
       "   curr_age  \n",
       "0      74.0  \n",
       "1      59.0  \n",
       "2      56.0  \n",
       "3      71.0  \n",
       "4      59.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics = pd.read_parquet('demographics.parquet', engine='fastparquet')\n",
    "demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subset_experiment_vars(X, experiment, lancet_vars):\n",
    "    lancets = pd.read_parquet('../../../randy/rfb/tidy_data/UKBiobank/dementia/lancet2024/lancet2024_preprocessed.parquet', engine = 'fastparquet')\n",
    "    demographics = pd.read_parquet('demographics.parquet', engine='fastparquet')\n",
    "\n",
    "    # experiment variables\n",
    "    experiment_vars = {\n",
    "        \"age_alone\": ['curr_age'],\n",
    "    \n",
    "        \"all_demographics\": \n",
    "            [\n",
    "                \"curr_age\",\n",
    "                \"31-0.0\",\n",
    "                #\"apoe\",\n",
    "                '6138-0.0', # 'max_educ_complete'\n",
    "                \"845-0.0\",\n",
    "                \"21000-0.0\",\n",
    "            ],\n",
    "        \"age_sex_lancet2024\":  \n",
    "            [\n",
    "                \"curr_age\",\n",
    "                \"31-0.0\",\n",
    "            ],\n",
    "        \"demographics_lancet2024\":\n",
    "            [\n",
    "                \"curr_age\",\n",
    "                \"31-0.0\",\n",
    "                #\"apoe\",\n",
    "                '6138-0.0',\n",
    "                \"845-0.0\",\n",
    "                \"21000-0.0\",\n",
    "            ]   \n",
    "    }\n",
    "    \n",
    "    lancet_vars, continuous_lancet_vars = get_lancet_vars()\n",
    "    \n",
    "    features_to_add = experiment_vars[experiment]\n",
    "\n",
    "    # subset demographics\n",
    "    demo_sub = demographics.loc[:, ['eid'] + features_to_add]    \n",
    "    X = X.merge(demo_sub, right_on='eid', left_on='IID', how = 'left')\n",
    "    X = X.drop(columns=['eid'])  # drop eid column after merge\n",
    "\n",
    "    # subset lancet_variables\n",
    "    if experiment in (\"age_sex_lancet2024\", \"demographics_lancet2024\"):\n",
    "        lancet_cols = [c for c in lancet_vars if c in lancets.columns]\n",
    "        lanc_sub = lancets.loc[:, ['eid'] + lancet_cols]\n",
    "        X = X.merge(lanc_sub, right_on='eid', left_on='IID', how='left')\n",
    "        X = X.drop(columns=['eid'])  # drop eid column after merge\n",
    "    \n",
    "    X = X.drop(columns=['IID'])\n",
    "\n",
    "    print(X.shape)\n",
    "    print(X.head())\n",
    "    print(X.columns)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_vars_for_scaling(\n",
    "    experiment, continuous_lancet_vars\n",
    "):\n",
    "    # choose the variables for scaling\n",
    "\n",
    "    continuous_cols = {\n",
    "        \"age_alone\": ['curr_age'],\n",
    "        \"all_demographics\": ['curr_age', \"845-0.0\"], \n",
    "        \"age_sex_lancet2024\": ['curr_age']\n",
    "        + continuous_lancet_vars,\n",
    "        \"demographics_lancet2024\": ['curr_age', \"845-0.0\"]\n",
    "        + continuous_lancet_vars,\n",
    "    }\n",
    "\n",
    "    if experiment in continuous_cols:\n",
    "        continuous_cols = continuous_cols[experiment]\n",
    "    else:\n",
    "        # output an error saying experiment is not in continuous_cols\n",
    "        print(\"Experiment not in continuous_cols\")\n",
    "        sys.exit()\n",
    "\n",
    "    return continuous_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_continuous_vars(X_train, X_test, continuous_cols):\n",
    "    \"\"\"\n",
    "    Scales the continuous variables in the training and test datasets using StandardScaler.\n",
    "    Used only with lrl1 model.\n",
    "    Parameters:\n",
    "    X_train (pd.DataFrame): The training dataset.\n",
    "    X_test (pd.DataFrame): The test dataset.\n",
    "    continuous_cols (list of str): List of column names corresponding to continuous variables to be scaled.\n",
    "    Returns:\n",
    "    tuple: A tuple containing the scaled training and test datasets (X_train, X_test).\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform only the continuous columns\n",
    "    scaler.fit(X_train[continuous_cols])\n",
    "    X_train.loc[:, continuous_cols] = scaler.transform(X_train[continuous_cols])\n",
    "    X_test.loc[:, continuous_cols] = scaler.transform(X_test[continuous_cols])\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_train_test(X, y, results_dir, fold_index):\n",
    "    # logging\n",
    "    logging.basicConfig(filename=f\"{results_dir}/logging.txt\", level=logging.INFO)\n",
    "    logging.info(\n",
    "            \"Running experiment %s with IDP index %d and fold index %d\",\n",
    "        )\n",
    "    logging.info(\"X shape: %s\", X.shape)\n",
    "    logging.info(\"y shape: %s\", y.shape)\n",
    "\n",
    "    # creating the folds\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234321)\n",
    "\n",
    "    logging.info(f\"Starting the k-fold cross-validation\")\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        if fold != fold_index:\n",
    "            continue\n",
    "        print('folds')\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def settings_automl(time_budget, metric, model):\n",
    "    \"\"\"\n",
    "    Generate settings for an AutoML classification task.\n",
    "    Parameters:\n",
    "    time_budget (int): The time budget for the AutoML process in seconds.\n",
    "    metric (str): The evaluation metric to be used (e.g., 'log_loss' 'accuracy', 'f1').\n",
    "    model (str): The model to be used in the AutoML process (e.g., 'lrl1').\n",
    "    region_index (int): The index of the region for logging purposes.\n",
    "    Returns:\n",
    "    dict: A dictionary containing the settings for the AutoML process.\n",
    "    \"\"\"\n",
    "    automl_settings = {\n",
    "            \"task\": \"classification\",\n",
    "            \"time_budget\": time_budget,\n",
    "            \"metric\": metric,\n",
    "            \"n_jobs\": -1,\n",
    "            \"eval_method\": \"cv\",\n",
    "            \"n_splits\": 5,\n",
    "            \"early_stop\": True,\n",
    "            \"log_training_metric\": True,\n",
    "            \"model_history\": True,\n",
    "            \"seed\": 1234321,\n",
    "            \"estimator_list\": [model],\n",
    "    }\n",
    "\n",
    "    return automl_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading datasets\n",
      "loaded_datasets\n",
      "       FID      IID  PAT  MAT  SEX  PHENOTYPE  rs113020870_C  rs2274119_C  \\\n",
      "0  3733928  3733928    0    0    1         -9            2.0          2.0   \n",
      "1  3607009  3607009    0    0    2         -9            2.0          2.0   \n",
      "2  2546462  2546462    0    0    2         -9            2.0          1.0   \n",
      "3  4886993  4886993    0    0    2         -9            1.0          2.0   \n",
      "4  2409738  2409738    0    0    2         -9            2.0          2.0   \n",
      "\n",
      "   rs3927683_A  rs1697421_C  ...  rs78179954_A  rs9614981_C  rs11913353_C  \\\n",
      "0          1.0            1  ...           2.0          2.0           2.0   \n",
      "1          0.0            1  ...           2.0          1.0           2.0   \n",
      "2          2.0            1  ...           2.0          1.0           2.0   \n",
      "3          1.0            1  ...           2.0          1.0           2.0   \n",
      "4          0.0            1  ...           2.0          1.0           2.0   \n",
      "\n",
      "   rs9616906_G  e3/e3  e2/e3  e3/e4  e2/e4  e4/e4  e2/e2  \n",
      "0            1      0      0      1      0      0      0  \n",
      "1            2      1      0      0      0      0      0  \n",
      "2            2      1      0      0      0      0      0  \n",
      "3            1      0      0      1      0      0      0  \n",
      "4            2      1      0      0      0      0      0  \n",
      "\n",
      "[5 rows x 743 columns]\n",
      "         FID      IID  PAT  MAT  SEX  PHENOTYPE  rs113020870_C  rs2274119_C  \\\n",
      "162  3324206  3324206    0    0    2         -9            2.0          2.0   \n",
      "253  2756205  2756205    0    0    1         -9            2.0          1.0   \n",
      "292  5246574  5246574    0    0    1         -9            2.0          2.0   \n",
      "403  3327019  3327019    0    0    2         -9            2.0          2.0   \n",
      "418  2469582  2469582    0    0    2         -9            2.0          2.0   \n",
      "\n",
      "     rs3927683_A  rs1697421_C  ...  rs9614981_C  rs11913353_C  rs9616906_G  \\\n",
      "162          1.0            1  ...          1.0           2.0            1   \n",
      "253          2.0            1  ...          2.0           2.0            2   \n",
      "292          1.0            2  ...          0.0           2.0            2   \n",
      "403          0.0            2  ...          NaN           2.0            2   \n",
      "418          2.0            2  ...          2.0           2.0            1   \n",
      "\n",
      "     e3/e3  e2/e3  e3/e4  e2/e4  e4/e4  e2/e2  groups  \n",
      "162      0      0      1      0      0      0       1  \n",
      "253      1      0      0      0      0      0       1  \n",
      "292      0      0      0      0      1      0       1  \n",
      "403      1      0      0      0      0      0       1  \n",
      "418      0      0      1      0      0      0       1  \n",
      "\n",
      "[5 rows x 744 columns]\n",
      "         IID  rs113020870_C  rs2274119_C  rs3927683_A  rs1697421_C  \\\n",
      "162  3324206            2.0          2.0          1.0            1   \n",
      "253  2756205            2.0          1.0          2.0            1   \n",
      "292  5246574            2.0          2.0          1.0            2   \n",
      "403  3327019            2.0          2.0          0.0            2   \n",
      "418  2469582            2.0          2.0          2.0            2   \n",
      "\n",
      "     rs7529220_T  rs760794442_CA  rs12041233_G  rs112751018_T  rs2819336_T  \\\n",
      "162          0.0             2.0             2            2.0          0.0   \n",
      "253          0.0             2.0             2            2.0          2.0   \n",
      "292          0.0             2.0             2            2.0          1.0   \n",
      "403          1.0             1.0             1            2.0          0.0   \n",
      "418          0.0             2.0             2            2.0          1.0   \n",
      "\n",
      "     ...  rs7522307_G  rs2466570_G  rs61848607_C  rs35771425_T  rs77303682_A  \\\n",
      "162  ...          2.0          2.0           2.0           2.0           2.0   \n",
      "253  ...          2.0          2.0           1.0           2.0           2.0   \n",
      "292  ...          2.0          2.0           2.0           2.0           2.0   \n",
      "403  ...          2.0          2.0           2.0           2.0           2.0   \n",
      "418  ...          1.0          2.0           2.0           2.0           2.0   \n",
      "\n",
      "     rs111033333_G  rs1338764_C  rs6673285_C  rs16846346_G  rs71537331_C  \n",
      "162              2          0.0          2.0           2.0           1.0  \n",
      "253              2          0.0          2.0           2.0           2.0  \n",
      "292              2          0.0          2.0           1.0           1.0  \n",
      "403              2          0.0          2.0           2.0           2.0  \n",
      "418              2          1.0          1.0           2.0           0.0  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "subsetting experiment variables\n",
      "(100, 54)\n",
      "   rs113020870_C  rs2274119_C  rs3927683_A  rs1697421_C  rs7529220_T  \\\n",
      "0            2.0          2.0          1.0            1          0.0   \n",
      "1            2.0          1.0          2.0            1          0.0   \n",
      "2            2.0          2.0          1.0            2          0.0   \n",
      "3            2.0          2.0          0.0            2          1.0   \n",
      "4            2.0          2.0          2.0            2          0.0   \n",
      "\n",
      "   rs760794442_CA  rs12041233_G  rs112751018_T  rs2819336_T  rs35599360_T  \\\n",
      "0             2.0             2            2.0          0.0           2.0   \n",
      "1             2.0             2            2.0          2.0           0.0   \n",
      "2             2.0             2            2.0          1.0           1.0   \n",
      "3             1.0             1            2.0          0.0           0.0   \n",
      "4             2.0             2            2.0          1.0           0.0   \n",
      "\n",
      "   ...  rs111033333_G  rs1338764_C  rs6673285_C  rs16846346_G  rs71537331_C  \\\n",
      "0  ...              2          0.0          2.0           2.0           1.0   \n",
      "1  ...              2          0.0          2.0           2.0           2.0   \n",
      "2  ...              2          0.0          2.0           1.0           1.0   \n",
      "3  ...              2          0.0          2.0           2.0           2.0   \n",
      "4  ...              2          1.0          1.0           2.0           0.0   \n",
      "\n",
      "   curr_age  31-0.0  6138-0.0  845-0.0  21000-0.0  \n",
      "0      76.0     0.0      -7.0     15.0     1003.0  \n",
      "1      82.0     1.0       5.0     15.0     1001.0  \n",
      "2      77.0     1.0      -7.0     16.0     1001.0  \n",
      "3      66.0     0.0       1.0      NaN     1001.0  \n",
      "4      78.0     0.0       5.0     15.0     1001.0  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "Index(['rs113020870_C', 'rs2274119_C', 'rs3927683_A', 'rs1697421_C',\n",
      "       'rs7529220_T', 'rs760794442_CA', 'rs12041233_G', 'rs112751018_T',\n",
      "       'rs2819336_T', 'rs35599360_T', 'rs7553439_C', 'rs112437613_C',\n",
      "       'rs2840677_A', 'rs2503185_A', 'rs34305371_G', 'rs11162433_G',\n",
      "       'rs551379_C', 'rs1008078_C', 'rs140076909_C', 'rs143083071_G',\n",
      "       'rs61813880_C', 'rs141749679_T', 'rs72700870_C', 'rs61812598_G',\n",
      "       'rs12726330_G', 'rs11591206_C', 'rs2099684_A', 'rs35821839_T',\n",
      "       'rs35089183_T', 'rs12561863_A', 'rs76403827_G', 'rs12733073_G',\n",
      "       'rs11588857_G', 'rs6696130_C', 'rs77333453_G', 'rs679515_T',\n",
      "       'rs17259038_A', 'rs2093760_A', 'rs4147104_A', 'rs7522307_G',\n",
      "       'rs2466570_G', 'rs61848607_C', 'rs35771425_T', 'rs77303682_A',\n",
      "       'rs111033333_G', 'rs1338764_C', 'rs6673285_C', 'rs16846346_G',\n",
      "       'rs71537331_C', 'curr_age', '31-0.0', '6138-0.0', '845-0.0',\n",
      "       '21000-0.0'],\n",
      "      dtype='object')\n",
      "Scaling data for lrl1 classifier\n",
      "Done retrieving continuous variables\n",
      "splitting\n",
      "folds\n",
      "training\n",
      "{'task': 'classification', 'time_budget': 3600, 'metric': 'log_loss', 'n_jobs': -1, 'eval_method': 'cv', 'n_splits': 5, 'early_stop': True, 'log_training_metric': True, 'model_history': True, 'seed': 1234321, 'estimator_list': ['lrl1']}\n",
      "\u001b[33m[flaml.automl.logger: 07-02 11:10:50] {1644} WARNING - With `model_history` set to `True` by default, all intermediate models are retained in memory, which may significantly increase memory usage and slow down training. Consider setting `model_history=False` to optimize memory and accelerate the training process.\u001b[0m\n",
      "[flaml.automl.logger: 07-02 11:10:50] {1752} INFO - task = classification\n",
      "[flaml.automl.logger: 07-02 11:10:50] {1763} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 07-02 11:10:50] {1862} INFO - Minimizing error metric: log_loss\n",
      "[flaml.automl.logger: 07-02 11:10:50] {1979} INFO - List of ML learners in AutoML Run: ['lrl1']\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 0, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2417} INFO - Estimated sufficient time budget=769s. Estimated necessary time budget=1s.\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.1s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 1, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.2s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 2, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.3s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 3, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.4s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 4, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.4s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 5, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.5s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 6, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.6s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 7, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.7s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2282} INFO - iteration 8, current learner lrl1\n",
      "[flaml.automl.logger: 07-02 11:10:50] {2466} INFO -  at 0.8s,\testimator lrl1's best error=0.6930,\tbest estimator lrl1's best error=0.6930\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 78\u001b[0m\n\u001b[1;32m     73\u001b[0m     results\u001b[38;5;241m.\u001b[39mto_parquet(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_labels_predictions.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# parse_args()\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# main() # this is the main function that runs the experiment\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(automl_settings)\n\u001b[1;32m     52\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mautoml_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# save the model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m best_model \u001b[38;5;241m=\u001b[39m automl\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mestimator\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/site-packages/flaml/automl/automl.py:2007\u001b[0m, in \u001b[0;36mAutoML.fit\u001b[0;34m(self, X_train, y_train, dataframe, label, metric, task, n_jobs, log_file_name, estimator_list, time_budget, max_iter, sample, ensemble, eval_method, log_type, model_history, split_ratio, n_splits, log_training_metric, mem_thres, pred_time_limit, train_time_limit, X_val, y_val, sample_weight_val, groups_val, groups, verbose, retrain_full, split_type, learner_selector, hpo_method, starting_points, seed, n_concurrent_trials, keep_search_state, preserve_checkpoint, early_stop, force_cancel, append_log, auto_augment, min_sample_size, use_ray, use_spark, free_mem_ratio, metric_constraints, custom_hp, time_col, cv_score_agg_func, skip_transform, mlflow_logging, fit_kwargs_by_estimator, mlflow_exp_name, **fit_kwargs)\u001b[0m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_estimator:\n\u001b[1;32m   2009\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/site-packages/flaml/automl/automl.py:2572\u001b[0m, in \u001b[0;36mAutoML._search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2566\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlflow_integration\u001b[38;5;241m.\u001b[39mrecord_state(\n\u001b[1;32m   2567\u001b[0m             automl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2568\u001b[0m             search_state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m   2569\u001b[0m             estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   2570\u001b[0m         )\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_ray \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_spark \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m-> 2572\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_parallel()\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/site-packages/flaml/automl/automl.py:2466\u001b[0m, in \u001b[0;36mAutoML._search_sequential\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlflow_integration:\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlflow_integration\u001b[38;5;241m.\u001b[39mrecord_state(\u001b[38;5;28mself\u001b[39m, search_state, estimator)\n\u001b[0;32m-> 2466\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m at \u001b[39;49m\u001b[38;5;132;43;01m{:.1f}\u001b[39;49;00m\u001b[38;5;124;43ms,\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43mestimator \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms best error=\u001b[39;49m\u001b[38;5;132;43;01m{:.4f}\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43mbest estimator \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms best error=\u001b[39;49m\u001b[38;5;132;43;01m{:.4f}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_from_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_best_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hpo_method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mtime_from_start \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_threshold \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_taken_best_iter)\n\u001b[1;32m   2482\u001b[0m ):\n\u001b[1;32m   2483\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   2484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll estimator hyperparameters local search has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverged at least once, and the total search time \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2486\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexceeds \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m times the time taken \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2487\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto find the best model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2488\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/logging/__init__.py:1477\u001b[0m, in \u001b[0;36mLogger.info\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;124;03mLog 'msg % args' with severity 'INFO'.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124;03mlogger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(INFO):\n\u001b[0;32m-> 1477\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINFO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/logging/__init__.py:1624\u001b[0m, in \u001b[0;36mLogger._log\u001b[0;34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001b[0m\n\u001b[1;32m   1621\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[1;32m   1622\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakeRecord(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, level, fn, lno, msg, args,\n\u001b[1;32m   1623\u001b[0m                          exc_info, func, extra, sinfo)\n\u001b[0;32m-> 1624\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/logging/__init__.py:1634\u001b[0m, in \u001b[0;36mLogger.handle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03mCall the handlers for the specified record.\u001b[39;00m\n\u001b[1;32m   1629\u001b[0m \n\u001b[1;32m   1630\u001b[0m \u001b[38;5;124;03mThis method is used for unpickled records received from a socket, as\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;124;03mwell as those created locally. Logger-level filtering is applied.\u001b[39;00m\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisabled) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter(record):\n\u001b[0;32m-> 1634\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallHandlers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/logging/__init__.py:1696\u001b[0m, in \u001b[0;36mLogger.callHandlers\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     found \u001b[38;5;241m=\u001b[39m found \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m record\u001b[38;5;241m.\u001b[39mlevelno \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m hdlr\u001b[38;5;241m.\u001b[39mlevel:\n\u001b[0;32m-> 1696\u001b[0m         \u001b[43mhdlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m c\u001b[38;5;241m.\u001b[39mpropagate:\n\u001b[1;32m   1698\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \u001b[38;5;66;03m#break out\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/logging/__init__.py:968\u001b[0m, in \u001b[0;36mHandler.handle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/logging/__init__.py:1104\u001b[0m, in \u001b[0;36mStreamHandler.emit\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;66;03m# issue 35046: merged two stream.writes into one.\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m     stream\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminator)\n\u001b[0;32m-> 1104\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRecursionError\u001b[39;00m:  \u001b[38;5;66;03m# See issue 36272\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/logging/__init__.py:1084\u001b[0m, in \u001b[0;36mStreamHandler.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1084\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/site-packages/ipykernel/iostream.py:604\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03msend will happen in the background thread\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mthread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m ):\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;66;03m# request flush on the background thread\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m     evt \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mEvent()\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/site-packages/ipykernel/iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     f()\n",
      "File \u001b[0;32m~/.conda/envs/lowpyAD/lib/python3.10/site-packages/zmq/sugar/socket.py:698\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    691\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[1;32m    692\u001b[0m             data,\n\u001b[1;32m    693\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[1;32m    694\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    695\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m--> 698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m_zmq.py:1081\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:1129\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:1397\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:169\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "    # parse the arguments\n",
    "    '''\n",
    "    (\n",
    "        experiment,\n",
    "        age_cutoff,\n",
    "        snps, \n",
    "        alzheimers_only\n",
    "    ) = parse_args()\n",
    "    '''\n",
    "    experiment = 'all_demographics' #(\"age_alone\" \"all_demographics\" \"age_sex_lancet2024\" \"demographics_lancet2024\")\n",
    "    age_cutoff = True\n",
    "    snps = 'apoe_stratified' # 'apoe', 'all_snps', 'LDE', 'polygenic_risk_score', 'apoe_stratified'\n",
    "    alzheimers_only = True\n",
    "    fold_index = 0\n",
    "    model = 'lrl1'\n",
    "    \n",
    "    print('loading datasets')\n",
    "    X, y = load_datasets(snps, age_cutoff, alzheimers_only)\n",
    "\n",
    "    results_dir = get_dir_path(age_cutoff, snps, alzheimers_only) + f\"/{fold_index}\"\n",
    "\n",
    "\n",
    "    # create results dir\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    # choosing experimental variables\n",
    "    print('subsetting experiment variables')\n",
    "    lancet_vars, continuous_lancet_vars = get_lancet_vars() \n",
    "    X = subset_experiment_vars(X, experiment, lancet_vars)\n",
    "    \n",
    "\n",
    "    if model == \"lrl1\":\n",
    "        print(\"Scaling data for lrl1 classifier\")\n",
    "        continuous_cols = continuous_vars_for_scaling(\n",
    "            experiment, continuous_lancet_vars\n",
    "        )\n",
    "        print(\"Done retrieving continuous variables\")\n",
    "\n",
    "    # split + train\n",
    "    print('splitting')\n",
    "    X_train, y_train, X_test, y_test = subset_train_test(X, y, results_dir, fold_index)\n",
    "    if model == \"lrl1\":\n",
    "        X_train, X_test = scale_continuous_vars(X_train, X_test, continuous_cols)\n",
    "\n",
    "    print('training')\n",
    "    automl = AutoML()\n",
    "    automl_settings = settings_automl(FLAML_TIME_BUDGET, metric=\"log_loss\", model=model)\n",
    "    print(automl_settings)\n",
    "\n",
    "    logging.info(f\"Saving the model: {datetime.now().time()}\")\n",
    "\n",
    "    automl.fit(X_train, y_train, **automl_settings)\n",
    "\n",
    "    # save the model\n",
    "    best_model = automl.model.estimator\n",
    "\n",
    "    # Save just the best model\n",
    "    joblib.dump(best_model, f\"{results_dir}/flaml_best_model.joblib\")\n",
    "\n",
    "    logging.info(f\"Saving the predictions: {datetime.now().time()}\")\n",
    "    # save the test set predictions\n",
    "    y_pred = automl.predict_proba(X_test)\n",
    "    results = pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred[:,1]})\n",
    "    results.to_parquet(\n",
    "        f\"{results_dir}/test_labels_predictions.parquet\", index=False\n",
    "    )\n",
    "\n",
    "    # save the train set predictions\n",
    "    y_pred = automl.predict_proba(X_train)\n",
    "    results = pd.DataFrame({\"y_train\": y_train, \"y_pred\": y_pred[:,1]})\n",
    "    results.to_parquet(\n",
    "        f\"{results_dir}/train_labels_predictions.parquet\", index=False\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # parse_args()\n",
    "    # main() # this is the main function that runs the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in use\n",
    "def average_feature_importances(): \n",
    "    # stats for all folds together\n",
    "    # merge folds together\n",
    "    merged_df = None\n",
    "\n",
    "    for fold_name, df in features.items():\n",
    "        # rename\n",
    "        df_renamed = df.rename(columns={'fi': f'fi_{fold_name}'})\n",
    "\n",
    "        if merged_df is None:\n",
    "            merged_df = df_renamed\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df_renamed, on='fnames', how='outer')\n",
    "\n",
    "    fi_cols = [col for col in merged_df.columns if col.startswith('fi_')]\n",
    "    merged_df['avg_fi'] = merged_df[fi_cols].mean(axis=1, skipna=True)\n",
    "    merged_df = merged_df.drop(columns=fi_cols)\n",
    "\n",
    "    # sort by average feature importance, check type\n",
    "    sorted_features = merged_df.sort_values(by='avg_fi', ascending=False)\n",
    "\n",
    "    sorted_features['fnames'] = sorted_features['fnames'].str.split(\"_\").str[0]\n",
    "    sorted_features['fnames'] = sorted_features['fnames'].astype('string')\n",
    "\n",
    "    # separate age and sex and apoe\n",
    "    non_snp_feats = sorted_features[sorted_features['fnames'].isin([\"e3/e3\", \"e2/e3\", \"e3/e4\", \"e2/e4\", \"e4/e4\",\"e2/e2\", 'age', 'sex'])]\n",
    "\n",
    "    # get snp names, filter by genes only present in fnames, then add to fnames\n",
    "    snps = get_snps()\n",
    "    snps = snps[snps['snp'].isin(sorted_features['fnames'])]\n",
    "    sorted_features = sorted_features.merge(snps, right_on = 'snp', left_on = 'fnames')\n",
    "    sorted_features = sorted_features.drop_duplicates(subset=['fnames'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lowpyAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
